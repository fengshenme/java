{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\10159\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.608 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score:0.986\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from pandas import Series,DataFrame\n",
    "# 获取远程数据集\n",
    "import pandas_datareader  as web\n",
    "from numpy.random import randn\n",
    "# 默认情况下，matplotlib不支持中文显示，我们需要进行一下设置。 主题设置\n",
    "# 设置字体,linux下用文泉驿字体windos下用SimHei字体(黑体),以支持中文显示\n",
    "mpl.rcParams[\"font.family\"] = [\"WenQuanYi Micro Hei Mono\"]\n",
    "# 设置在中文字体时，能够正常的显示负号（-）。\n",
    "mpl.rcParams[\"axes.unicode_minus\"] = False\n",
    "from sklearn.base import TransformerMixin\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import jieba # 因为是中文所以用结巴分词,\n",
    "class NLTKBOW(TransformerMixin):\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    def transform(self,X):\n",
    "        return [{word:True for word in jieba.cut(document, cut_all=False)} for document in X]\n",
    "df = pd.read_csv(\"./data/weibo/weibo_python_utf8.csv\",encoding='utf-8',sep='`',index_col='Unnamed: 0')\n",
    "tweets = df['text'].values\n",
    "labels = df['labels'].values\n",
    "labels[:5] = 0\n",
    "pipeline = Pipeline([('bag-of-words',NLTKBOW()),\n",
    "                     ('vectorizer',DictVectorizer()),\n",
    "                     ('naive-bayes',BernoulliNB())])\n",
    "scores = cross_val_score(pipeline,tweets,labels,scoring='f1')\n",
    "print('score:{:.3f}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(tweets,labels)\n",
    "nb = model.named_steps['naive-bayes']\n",
    "feature_probabilities = nb.feature_log_prob_ \n",
    "top_features = np.argsort(-feature_probabilities[1])[:50]\n",
    "dv = model.named_steps['vectorizer']\n",
    "for i,feature_index in enumerate(top_features):\n",
    "    print(i,dv.feature_names_[feature_index],np.exp(feature_probabilities[1][feature_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "output_filename = os.path.join(os.path.expanduser('./'),'models','twitter','python_context.pkl')\n",
    "# 接着,用joblib库的dump函数,与json库的dump函数功能类似.参数为模型本身(怕你忘了,捎带提下,模型名称为model)和输出文件名\n",
    "joblib.dump(model,output_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
